{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqloAPIScraper:\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        self.products_data = []\n",
    "\n",
    "    def fetch_data(self, page: int):\n",
    "        \"\"\"Fetch product data from Uniqlo's API for a specific page.\"\"\"\n",
    "        offset = (page - 1) * 10  # API paginates by 10 items per page\n",
    "        api_url = f\"{self.base_url}?offset={offset}&limit=10\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract products from the `result` field\n",
    "            products = data.get(\"result\", {}).get(\"items\", [])\n",
    "            return products\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data for page {page}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scrape(self):\n",
    "        \"\"\"Collect product data across multiple pages.\"\"\"\n",
    "        page = 1\n",
    "        while True:\n",
    "            print(f\"Fetching data for page {page}...\")\n",
    "            products = self.fetch_data(page)\n",
    "\n",
    "            if not products:  # Stop if no products are found on the page\n",
    "                print(f\"No products found on page {page}. Stopping collection.\")\n",
    "                break\n",
    "\n",
    "            for product in products:\n",
    "                product_info = {\n",
    "                    \"Product Name\": product.get(\"name\", \"N/A\"),\n",
    "                    \"Price\": product.get(\"prices\", {}).get(\"base\", {}).get(\"value\", \"N/A\"),\n",
    "                    \"Currency\": product.get(\"prices\", {}).get(\"base\", {}).get(\"currency\", {}).get(\"code\", \"N/A\"),\n",
    "                    \"Colors\": [color.get(\"name\", \"N/A\") for color in product.get(\"colors\", [])],\n",
    "                    \"URL\": f'https://www.uniqlo.com/vn/vi/products/{product.get(\"productId\", \"N/A\")}?colorCode={product.get(\"colors\", [{}])[0].get(\"code\", \"N/A\")}&sizeCode={product.get(\"sizes\", [{}])[0].get(\"code\", \"N/A\")}',\n",
    "                    \"Rating (Average)\": product.get(\"rating\", {}).get(\"average\", \"N/A\"),\n",
    "                    \"Total Ratings\": sum(product.get(\"rating\", {}).get(\"rateCount\", {}).values()),\n",
    "                }\n",
    "                self.products_data.append(product_info)\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            # Add a delay to avoid overloading the server\n",
    "            time.sleep(2)\n",
    "\n",
    "        print(f\"Collection complete. Total products collected: {len(self.products_data)}\")\n",
    "\n",
    "    def save_to_csv(self, filename: str):\n",
    "        \"\"\"Save the collected data to a CSV file.\"\"\"\n",
    "        if not self.products_data:\n",
    "            print(\"No data to save!\")\n",
    "            return\n",
    "        df = pd.DataFrame(self.products_data)\n",
    "        df.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "        print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for page 1...\n",
      "Fetching data for page 2...\n",
      "Fetching data for page 3...\n",
      "Fetching data for page 4...\n",
      "Fetching data for page 5...\n",
      "Fetching data for page 6...\n",
      "Fetching data for page 7...\n",
      "Fetching data for page 8...\n",
      "Fetching data for page 9...\n",
      "Fetching data for page 10...\n",
      "Fetching data for page 11...\n",
      "Fetching data for page 12...\n",
      "Fetching data for page 13...\n",
      "Fetching data for page 14...\n",
      "Fetching data for page 15...\n",
      "Fetching data for page 16...\n",
      "Fetching data for page 17...\n",
      "Fetching data for page 18...\n",
      "Fetching data for page 19...\n",
      "Fetching data for page 20...\n",
      "Fetching data for page 21...\n",
      "Fetching data for page 22...\n",
      "Fetching data for page 23...\n",
      "Fetching data for page 24...\n",
      "Fetching data for page 25...\n",
      "Fetching data for page 26...\n",
      "Fetching data for page 27...\n",
      "Fetching data for page 28...\n",
      "Fetching data for page 29...\n",
      "Fetching data for page 30...\n",
      "Fetching data for page 31...\n",
      "Fetching data for page 32...\n",
      "Fetching data for page 33...\n",
      "Fetching data for page 34...\n",
      "Fetching data for page 35...\n",
      "Fetching data for page 36...\n",
      "Fetching data for page 37...\n",
      "Fetching data for page 38...\n",
      "Fetching data for page 39...\n",
      "Fetching data for page 40...\n",
      "Fetching data for page 41...\n",
      "Fetching data for page 42...\n",
      "Fetching data for page 43...\n",
      "Fetching data for page 44...\n",
      "Fetching data for page 45...\n",
      "Fetching data for page 46...\n",
      "Fetching data for page 47...\n",
      "Fetching data for page 48...\n",
      "Fetching data for page 49...\n",
      "Fetching data for page 50...\n",
      "Fetching data for page 51...\n",
      "Fetching data for page 52...\n",
      "Fetching data for page 53...\n",
      "Fetching data for page 54...\n",
      "Fetching data for page 55...\n",
      "Fetching data for page 56...\n",
      "Fetching data for page 57...\n",
      "Fetching data for page 58...\n",
      "Fetching data for page 59...\n",
      "Fetching data for page 60...\n",
      "Fetching data for page 61...\n",
      "Fetching data for page 62...\n",
      "Fetching data for page 63...\n",
      "Fetching data for page 64...\n",
      "Fetching data for page 65...\n",
      "Fetching data for page 66...\n",
      "Fetching data for page 67...\n",
      "Fetching data for page 68...\n",
      "Fetching data for page 69...\n",
      "Fetching data for page 70...\n",
      "Fetching data for page 71...\n",
      "Fetching data for page 72...\n",
      "Fetching data for page 73...\n",
      "Fetching data for page 74...\n",
      "Fetching data for page 75...\n",
      "Fetching data for page 76...\n",
      "Fetching data for page 77...\n",
      "Fetching data for page 78...\n",
      "Fetching data for page 79...\n",
      "Fetching data for page 80...\n",
      "Fetching data for page 81...\n",
      "Fetching data for page 82...\n",
      "Fetching data for page 83...\n",
      "Fetching data for page 84...\n",
      "Fetching data for page 85...\n",
      "Fetching data for page 86...\n",
      "Fetching data for page 87...\n",
      "Fetching data for page 88...\n",
      "Fetching data for page 89...\n",
      "Fetching data for page 90...\n",
      "Fetching data for page 91...\n",
      "Fetching data for page 92...\n",
      "Fetching data for page 93...\n",
      "Fetching data for page 94...\n",
      "Fetching data for page 95...\n",
      "Fetching data for page 96...\n",
      "Fetching data for page 97...\n",
      "Fetching data for page 98...\n",
      "Fetching data for page 99...\n",
      "No products found on page 99. Stopping collection.\n",
      "Collection complete. Total products collected: 974\n",
      "Data has been saved to uniqlo_products.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.uniqlo.com/vn/api/commerce/v3/vi/products\"\n",
    "    \n",
    "    scraper = UniqloAPIScraper(base_url)\n",
    "    scraper.scrape()\n",
    "    scraper.save_to_csv(\"uniqlo_products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
